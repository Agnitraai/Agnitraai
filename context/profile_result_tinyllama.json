{
  "model": "tinyllama.pt",
  "device": "NVIDIA A100-SXM4-40GB",
  "meta": {
    "python_version": "3.12.11",
    "torch_version": "2.8.0+cu126",
    "cuda_available": true,
    "cuda_version": "12.6",
    "cudnn_version": 91002,
    "device_name": "NVIDIA A100-SXM4-40GB",
    "device_capability": [
      8,
      0
    ],
    "model_num_parameters": 164544
  },
  "gpu": {
    "gpu_utilisation": 0,
    "memory_utilisation": 0,
    "power_watts": 50.602,
    "temperature_c": 34.0,
    "fan_speed_pct": null,
    "name": "NVIDIA A100-SXM4-40GB",
    "driver_version": "550.54.15",
    "vram_total_mb": 40960.0,
    "vram_used_mb": 1537.75,
    "vram_free_mb": 39422.25,
    "cuda_allocated_mb": 8.88330078125,
    "cuda_reserved_mb": 22.0
  },
  "summary": {
    "num_events": 39,
    "total_cuda_time_ms": 0.0,
    "total_cpu_time_ms": 0.015622766000000003,
    "top_by_time": [
      {
        "name": "forward",
        "cuda_ms": 0.0,
        "cpu_ms": 0.006338834999999999,
        "count": 1,
        "cuda_mem": 0,
        "cpu_mem": 0
      },
      {
        "name": "aten::slice",
        "cuda_ms": 0.0,
        "cpu_ms": 0.0002927240000000093,
        "count": 3,
        "cuda_mem": 0,
        "cpu_mem": 0
      },
      {
        "name": "aten::as_strided",
        "cuda_ms": 0.0,
        "cpu_ms": 7.778900000002614e-05,
        "count": 23,
        "cuda_mem": 0,
        "cpu_mem": 0
      },
      {
        "name": "aten::add",
        "cuda_ms": 0.0,
        "cpu_ms": 0.0027530229999999938,
        "count": 3,
        "cuda_mem": 0,
        "cpu_mem": 0
      },
      {
        "name": "Unrecognized",
        "cuda_ms": 0.0,
        "cpu_ms": 0.0021985480000000024,
        "count": 1,
        "cuda_mem": 0,
        "cpu_mem": 0
      },
      {
        "name": "cudaLaunchKernel",
        "cuda_ms": 0.0,
        "cpu_ms": 0.00026434300000000076,
        "count": 12,
        "cuda_mem": 0,
        "cpu_mem": 0
      },
      {
        "name": "void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, std::array<char*, 3ul> >(int, at::native::CUDAFunctor_add<float>, std::array<char*, 3ul>)",
        "cuda_ms": 0.0,
        "cpu_ms": 0.0,
        "count": 3,
        "cuda_mem": 0,
        "cpu_mem": 0
      },
      {
        "name": "aten::transpose",
        "cuda_ms": 0.0,
        "cpu_ms": 0.00011493199999999342,
        "count": 14,
        "cuda_mem": 0,
        "cpu_mem": 0
      },
      {
        "name": "aten::linear",
        "cuda_ms": 0.0,
        "cpu_ms": 0.0007792989999999918,
        "count": 4,
        "cuda_mem": 0,
        "cpu_mem": 0
      },
      {
        "name": "aten::reshape",
        "cuda_ms": 0.0,
        "cpu_ms": 4.986699999999837e-05,
        "count": 3,
        "cuda_mem": 0,
        "cpu_mem": 0
      },
      {
        "name": "aten::view",
        "cuda_ms": 0.0,
        "cpu_ms": 9.012999999999738e-05,
        "count": 19,
        "cuda_mem": 0,
        "cpu_mem": 0
      },
      {
        "name": "aten::t",
        "cuda_ms": 0.0,
        "cpu_ms": 0.00012540299999999843,
        "count": 4,
        "cuda_mem": 0,
        "cpu_mem": 0
      },
      {
        "name": "aten::addmm",
        "cuda_ms": 0.0,
        "cpu_ms": 0.00050125,
        "count": 4,
        "cuda_mem": 0,
        "cpu_mem": 0
      },
      {
        "name": "void gemmSN_TN_kernel<float, 128, 16, 2, 4, 8, 9, false, cublasGemvTensorStridedBatched<float const>, cublasGemvTensorStridedBatched<float const>, cublasGemvTensorStridedBatched<float> >(cublasGemmSmallNParams<cublasGemvTensorStridedBatched<float const>, cublasGemvTensorStridedBatched<float const>, cublasGemvTensorStridedBatched<float>, float>)",
        "cuda_ms": 0.0,
        "cpu_ms": 0.0,
        "count": 4,
        "cuda_mem": 0,
        "cpu_mem": 0
      },
      {
        "name": "aten::unflatten",
        "cuda_ms": 0.0,
        "cpu_ms": 3.1968000000000755e-05,
        "count": 1,
        "cuda_mem": 0,
        "cpu_mem": 0
      }
    ],
    "top_by_mem": [
      {
        "name": "forward",
        "cuda_mem": 0,
        "cpu_mem": 0
      },
      {
        "name": "aten::slice",
        "cuda_mem": 0,
        "cpu_mem": 0
      },
      {
        "name": "aten::as_strided",
        "cuda_mem": 0,
        "cpu_mem": 0
      },
      {
        "name": "aten::add",
        "cuda_mem": 0,
        "cpu_mem": 0
      },
      {
        "name": "Unrecognized",
        "cuda_mem": 0,
        "cpu_mem": 0
      },
      {
        "name": "cudaLaunchKernel",
        "cuda_mem": 0,
        "cpu_mem": 0
      },
      {
        "name": "void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, std::array<char*, 3ul> >(int, at::native::CUDAFunctor_add<float>, std::array<char*, 3ul>)",
        "cuda_mem": 0,
        "cpu_mem": 0
      },
      {
        "name": "aten::transpose",
        "cuda_mem": 0,
        "cpu_mem": 0
      },
      {
        "name": "aten::linear",
        "cuda_mem": 0,
        "cpu_mem": 0
      },
      {
        "name": "aten::reshape",
        "cuda_mem": 0,
        "cpu_mem": 0
      },
      {
        "name": "aten::view",
        "cuda_mem": 0,
        "cpu_mem": 0
      },
      {
        "name": "aten::t",
        "cuda_mem": 0,
        "cpu_mem": 0
      },
      {
        "name": "aten::addmm",
        "cuda_mem": 0,
        "cpu_mem": 0
      },
      {
        "name": "void gemmSN_TN_kernel<float, 128, 16, 2, 4, 8, 9, false, cublasGemvTensorStridedBatched<float const>, cublasGemvTensorStridedBatched<float const>, cublasGemvTensorStridedBatched<float> >(cublasGemmSmallNParams<cublasGemvTensorStridedBatched<float const>, cublasGemvTensorStridedBatched<float const>, cublasGemvTensorStridedBatched<float>, float>)",
        "cuda_mem": 0,
        "cpu_mem": 0
      },
      {
        "name": "aten::unflatten",
        "cuda_mem": 0,
        "cpu_mem": 0
      }
    ]
  },
  "model_profile": {
    "layers": [
      {
        "name": "",
        "type": "_Wrapper",
        "class_path": "cli.profile._Wrapper",
        "parameters": [],
        "buffers": [],
        "attributes": {},
        "input_shapes": [
          [
            1,
            16,
            64
          ]
        ],
        "output_shapes": [
          [
            1,
            16,
            64
          ]
        ],
        "output_dtype": "float32"
      },
      {
        "name": "inner",
        "type": "RecursiveScriptModule",
        "class_path": "torch.jit._script.RecursiveScriptModule",
        "parameters": [],
        "buffers": [],
        "attributes": {}
      },
      {
        "name": "inner.embed",
        "type": "RecursiveScriptModule",
        "class_path": "torch.jit._script.RecursiveScriptModule",
        "parameters": [
          {
            "name": "weight",
            "shape": [
              2048,
              64
            ],
            "numel": 131072,
            "dtype": "float32",
            "device": "cuda:0",
            "requires_grad": true,
            "kind": "weight"
          }
        ],
        "buffers": [],
        "attributes": {}
      },
      {
        "name": "inner.pos",
        "type": "RecursiveScriptModule",
        "class_path": "torch.jit._script.RecursiveScriptModule",
        "parameters": [],
        "buffers": [
          {
            "name": "pe",
            "shape": [
              1,
              512,
              64
            ],
            "numel": 32768,
            "dtype": "float32",
            "device": "cuda:0",
            "kind": "buffer"
          }
        ],
        "attributes": {}
      },
      {
        "name": "inner.attn",
        "type": "RecursiveScriptModule",
        "class_path": "torch.jit._script.RecursiveScriptModule",
        "parameters": [
          {
            "name": "in_proj_weight",
            "shape": [
              192,
              64
            ],
            "numel": 12288,
            "dtype": "float32",
            "device": "cuda:0",
            "requires_grad": true,
            "kind": "parameter"
          },
          {
            "name": "in_proj_bias",
            "shape": [
              192
            ],
            "numel": 192,
            "dtype": "float32",
            "device": "cuda:0",
            "requires_grad": true,
            "kind": "parameter"
          }
        ],
        "buffers": [],
        "attributes": {}
      },
      {
        "name": "inner.attn.out_proj",
        "type": "RecursiveScriptModule",
        "class_path": "torch.jit._script.RecursiveScriptModule",
        "parameters": [
          {
            "name": "weight",
            "shape": [
              64,
              64
            ],
            "numel": 4096,
            "dtype": "float32",
            "device": "cuda:0",
            "requires_grad": true,
            "kind": "weight"
          },
          {
            "name": "bias",
            "shape": [
              64
            ],
            "numel": 64,
            "dtype": "float32",
            "device": "cuda:0",
            "requires_grad": true,
            "kind": "bias"
          }
        ],
        "buffers": [],
        "attributes": {}
      },
      {
        "name": "inner.norm1",
        "type": "RecursiveScriptModule",
        "class_path": "torch.jit._script.RecursiveScriptModule",
        "parameters": [
          {
            "name": "weight",
            "shape": [
              64
            ],
            "numel": 64,
            "dtype": "float32",
            "device": "cuda:0",
            "requires_grad": true,
            "kind": "weight"
          },
          {
            "name": "bias",
            "shape": [
              64
            ],
            "numel": 64,
            "dtype": "float32",
            "device": "cuda:0",
            "requires_grad": true,
            "kind": "bias"
          }
        ],
        "buffers": [],
        "attributes": {}
      },
      {
        "name": "inner.mlp",
        "type": "RecursiveScriptModule",
        "class_path": "torch.jit._script.RecursiveScriptModule",
        "parameters": [],
        "buffers": [],
        "attributes": {}
      },
      {
        "name": "inner.mlp.0",
        "type": "RecursiveScriptModule",
        "class_path": "torch.jit._script.RecursiveScriptModule",
        "parameters": [
          {
            "name": "weight",
            "shape": [
              128,
              64
            ],
            "numel": 8192,
            "dtype": "float32",
            "device": "cuda:0",
            "requires_grad": true,
            "kind": "weight"
          },
          {
            "name": "bias",
            "shape": [
              128
            ],
            "numel": 128,
            "dtype": "float32",
            "device": "cuda:0",
            "requires_grad": true,
            "kind": "bias"
          }
        ],
        "buffers": [],
        "attributes": {}
      },
      {
        "name": "inner.mlp.1",
        "type": "RecursiveScriptModule",
        "class_path": "torch.jit._script.RecursiveScriptModule",
        "parameters": [],
        "buffers": [],
        "attributes": {}
      },
      {
        "name": "inner.mlp.2",
        "type": "RecursiveScriptModule",
        "class_path": "torch.jit._script.RecursiveScriptModule",
        "parameters": [
          {
            "name": "weight",
            "shape": [
              64,
              128
            ],
            "numel": 8192,
            "dtype": "float32",
            "device": "cuda:0",
            "requires_grad": true,
            "kind": "weight"
          },
          {
            "name": "bias",
            "shape": [
              64
            ],
            "numel": 64,
            "dtype": "float32",
            "device": "cuda:0",
            "requires_grad": true,
            "kind": "bias"
          }
        ],
        "buffers": [],
        "attributes": {}
      },
      {
        "name": "inner.norm2",
        "type": "RecursiveScriptModule",
        "class_path": "torch.jit._script.RecursiveScriptModule",
        "parameters": [
          {
            "name": "weight",
            "shape": [
              64
            ],
            "numel": 64,
            "dtype": "float32",
            "device": "cuda:0",
            "requires_grad": true,
            "kind": "weight"
          },
          {
            "name": "bias",
            "shape": [
              64
            ],
            "numel": 64,
            "dtype": "float32",
            "device": "cuda:0",
            "requires_grad": true,
            "kind": "bias"
          }
        ],
        "buffers": [],
        "attributes": {}
      }
    ],
    "parameter_count_total": 164544,
    "parameter_count_trainable": 164544,
    "parameter_count_frozen": 0,
    "parameter_dtypes": {
      "float32": 164544
    },
    "buffer_dtypes": {
      "float32": 32768
    },
    "model_dtype": "float32"
  },
  "training_summary": {
    "num_events": 120,
    "total_cuda_time_ms": 0.0,
    "total_cpu_time_ms": 0.5226928860000002,
    "top_by_time": [
      {
        "name": "Optimizer.zero_grad#SGD.zero_grad",
        "cuda_ms": 0.0,
        "cpu_ms": 0.0002244529999999994,
        "count": 3,
        "cuda_mem": 0,
        "cpu_mem": 0
      },
      {
        "name": "forward",
        "cuda_ms": 0.0,
        "cpu_ms": 0.005412666999999988,
        "count": 3,
        "cuda_mem": 0,
        "cpu_mem": 0
      },
      {
        "name": "aten::slice",
        "cuda_ms": 0.0,
        "cpu_ms": 8.300400000000274e-05,
        "count": 9,
        "cuda_mem": 0,
        "cpu_mem": 0
      },
      {
        "name": "aten::as_strided",
        "cuda_ms": 0.0,
        "cpu_ms": 0.00022269900000002974,
        "count": 162,
        "cuda_mem": 0,
        "cpu_mem": 0
      },
      {
        "name": "aten::add",
        "cuda_ms": 0.0,
        "cpu_ms": 0.0026495380000000042,
        "count": 12,
        "cuda_mem": 0,
        "cpu_mem": 0
      },
      {
        "name": "Unrecognized",
        "cuda_ms": 0.0,
        "cpu_ms": 0.002233114,
        "count": 1,
        "cuda_mem": 0,
        "cpu_mem": 0
      },
      {
        "name": "cudaLaunchKernel",
        "cuda_ms": 0.0,
        "cpu_ms": 0.08137470300000003,
        "count": 135,
        "cuda_mem": 0,
        "cpu_mem": 0
      },
      {
        "name": "void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, std::array<char*, 3ul> >(int, at::native::CUDAFunctor_add<float>, std::array<char*, 3ul>)",
        "cuda_ms": 0.0,
        "cpu_ms": 0.0,
        "count": 18,
        "cuda_mem": 0,
        "cpu_mem": 0
      },
      {
        "name": "aten::transpose",
        "cuda_ms": 0.0,
        "cpu_ms": 0.0004965530000000135,
        "count": 111,
        "cuda_mem": 0,
        "cpu_mem": 0
      },
      {
        "name": "aten::linear",
        "cuda_ms": 0.0,
        "cpu_ms": 0.0009721309999999931,
        "count": 12,
        "cuda_mem": 0,
        "cpu_mem": 0
      },
      {
        "name": "aten::reshape",
        "cuda_ms": 0.0,
        "cpu_ms": 0.0004937180000000695,
        "count": 57,
        "cuda_mem": 0,
        "cpu_mem": 0
      },
      {
        "name": "aten::view",
        "cuda_ms": 0.0,
        "cpu_ms": 0.0002951539999999427,
        "count": 105,
        "cuda_mem": 0,
        "cpu_mem": 0
      },
      {
        "name": "aten::t",
        "cuda_ms": 0.0,
        "cpu_ms": 0.0003022069999999617,
        "count": 39,
        "cuda_mem": 0,
        "cpu_mem": 0
      },
      {
        "name": "aten::addmm",
        "cuda_ms": 0.0,
        "cpu_ms": 0.0006638419999999829,
        "count": 12,
        "cuda_mem": 0,
        "cpu_mem": 0
      },
      {
        "name": "void gemmSN_TN_kernel<float, 128, 16, 2, 4, 8, 9, false, cublasGemvTensorStridedBatched<float const>, cublasGemvTensorStridedBatched<float const>, cublasGemvTensorStridedBatched<float> >(cublasGemmSmallNParams<cublasGemvTensorStridedBatched<float const>, cublasGemvTensorStridedBatched<float const>, cublasGemvTensorStridedBatched<float>, float>)",
        "cuda_ms": 0.0,
        "cpu_ms": 0.0,
        "count": 12,
        "cuda_mem": 0,
        "cpu_mem": 0
      }
    ],
    "top_by_mem": [
      {
        "name": "Optimizer.zero_grad#SGD.zero_grad",
        "cuda_mem": 0,
        "cpu_mem": 0
      },
      {
        "name": "forward",
        "cuda_mem": 0,
        "cpu_mem": 0
      },
      {
        "name": "aten::slice",
        "cuda_mem": 0,
        "cpu_mem": 0
      },
      {
        "name": "aten::as_strided",
        "cuda_mem": 0,
        "cpu_mem": 0
      },
      {
        "name": "aten::add",
        "cuda_mem": 0,
        "cpu_mem": 0
      },
      {
        "name": "Unrecognized",
        "cuda_mem": 0,
        "cpu_mem": 0
      },
      {
        "name": "cudaLaunchKernel",
        "cuda_mem": 0,
        "cpu_mem": 0
      },
      {
        "name": "void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, std::array<char*, 3ul> >(int, at::native::CUDAFunctor_add<float>, std::array<char*, 3ul>)",
        "cuda_mem": 0,
        "cpu_mem": 0
      },
      {
        "name": "aten::transpose",
        "cuda_mem": 0,
        "cpu_mem": 0
      },
      {
        "name": "aten::linear",
        "cuda_mem": 0,
        "cpu_mem": 0
      },
      {
        "name": "aten::reshape",
        "cuda_mem": 0,
        "cpu_mem": 0
      },
      {
        "name": "aten::view",
        "cuda_mem": 0,
        "cpu_mem": 0
      },
      {
        "name": "aten::t",
        "cuda_mem": 0,
        "cpu_mem": 0
      },
      {
        "name": "aten::addmm",
        "cuda_mem": 0,
        "cpu_mem": 0
      },
      {
        "name": "void gemmSN_TN_kernel<float, 128, 16, 2, 4, 8, 9, false, cublasGemvTensorStridedBatched<float const>, cublasGemvTensorStridedBatched<float const>, cublasGemvTensorStridedBatched<float> >(cublasGemmSmallNParams<cublasGemvTensorStridedBatched<float const>, cublasGemvTensorStridedBatched<float const>, cublasGemvTensorStridedBatched<float>, float>)",
        "cuda_mem": 0,
        "cpu_mem": 0
      }
    ]
  },
  "validation_summary": {
    "num_events": 32,
    "total_cuda_time_ms": 0.0,
    "total_cpu_time_ms": 0.014547158,
    "top_by_time": [
      {
        "name": "forward",
        "cuda_ms": 0.0,
        "cpu_ms": 0.005626375,
        "count": 2,
        "cuda_mem": 0,
        "cpu_mem": 0
      },
      {
        "name": "aten::slice",
        "cuda_ms": 0.0,
        "cpu_ms": 7.982399999999984e-05,
        "count": 12,
        "cuda_mem": 0,
        "cpu_mem": 0
      },
      {
        "name": "aten::as_strided",
        "cuda_ms": 0.0,
        "cpu_ms": 3.9883000000001177e-05,
        "count": 42,
        "cuda_mem": 0,
        "cpu_mem": 0
      },
      {
        "name": "aten::add",
        "cuda_ms": 0.0,
        "cpu_ms": 0.002471550999999999,
        "count": 6,
        "cuda_mem": 0,
        "cpu_mem": 0
      },
      {
        "name": "Unrecognized",
        "cuda_ms": 0.0,
        "cpu_ms": 0.002187031,
        "count": 1,
        "cuda_mem": 0,
        "cpu_mem": 0
      },
      {
        "name": "cudaLaunchKernel",
        "cuda_ms": 0.0,
        "cpu_ms": 0.00026263099999999896,
        "count": 22,
        "cuda_mem": 0,
        "cpu_mem": 0
      },
      {
        "name": "void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, std::array<char*, 3ul> >(int, at::native::CUDAFunctor_add<float>, std::array<char*, 3ul>)",
        "cuda_ms": 0.0,
        "cpu_ms": 0.0,
        "count": 6,
        "cuda_mem": 0,
        "cpu_mem": 0
      },
      {
        "name": "aten::_native_multi_head_attention",
        "cuda_ms": 0.0,
        "cpu_ms": 0.0007759450000000002,
        "count": 2,
        "cuda_mem": 0,
        "cpu_mem": 0
      },
      {
        "name": "aten::view",
        "cuda_ms": 0.0,
        "cpu_ms": 6.078399999999919e-05,
        "count": 38,
        "cuda_mem": 0,
        "cpu_mem": 0
      },
      {
        "name": "aten::transpose",
        "cuda_ms": 0.0,
        "cpu_ms": 8.565599999999858e-05,
        "count": 30,
        "cuda_mem": 0,
        "cpu_mem": 0
      },
      {
        "name": "aten::linear",
        "cuda_ms": 0.0,
        "cpu_ms": 0.0005409120000000003,
        "count": 8,
        "cuda_mem": 0,
        "cpu_mem": 0
      },
      {
        "name": "aten::reshape",
        "cuda_ms": 0.0,
        "cpu_ms": 4.788600000000042e-05,
        "count": 10,
        "cuda_mem": 0,
        "cpu_mem": 0
      },
      {
        "name": "aten::t",
        "cuda_ms": 0.0,
        "cpu_ms": 4.0494999999999435e-05,
        "count": 8,
        "cuda_mem": 0,
        "cpu_mem": 0
      },
      {
        "name": "aten::addmm",
        "cuda_ms": 0.0,
        "cpu_ms": 0.0003613110000000006,
        "count": 8,
        "cuda_mem": 0,
        "cpu_mem": 0
      },
      {
        "name": "void gemmSN_TN_kernel<float, 128, 16, 2, 4, 8, 9, false, cublasGemvTensorStridedBatched<float const>, cublasGemvTensorStridedBatched<float const>, cublasGemvTensorStridedBatched<float> >(cublasGemmSmallNParams<cublasGemvTensorStridedBatched<float const>, cublasGemvTensorStridedBatched<float const>, cublasGemvTensorStridedBatched<float>, float>)",
        "cuda_ms": 0.0,
        "cpu_ms": 0.0,
        "count": 8,
        "cuda_mem": 0,
        "cpu_mem": 0
      }
    ],
    "top_by_mem": [
      {
        "name": "forward",
        "cuda_mem": 0,
        "cpu_mem": 0
      },
      {
        "name": "aten::slice",
        "cuda_mem": 0,
        "cpu_mem": 0
      },
      {
        "name": "aten::as_strided",
        "cuda_mem": 0,
        "cpu_mem": 0
      },
      {
        "name": "aten::add",
        "cuda_mem": 0,
        "cpu_mem": 0
      },
      {
        "name": "Unrecognized",
        "cuda_mem": 0,
        "cpu_mem": 0
      },
      {
        "name": "cudaLaunchKernel",
        "cuda_mem": 0,
        "cpu_mem": 0
      },
      {
        "name": "void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, std::array<char*, 3ul> >(int, at::native::CUDAFunctor_add<float>, std::array<char*, 3ul>)",
        "cuda_mem": 0,
        "cpu_mem": 0
      },
      {
        "name": "aten::_native_multi_head_attention",
        "cuda_mem": 0,
        "cpu_mem": 0
      },
      {
        "name": "aten::view",
        "cuda_mem": 0,
        "cpu_mem": 0
      },
      {
        "name": "aten::transpose",
        "cuda_mem": 0,
        "cpu_mem": 0
      },
      {
        "name": "aten::linear",
        "cuda_mem": 0,
        "cpu_mem": 0
      },
      {
        "name": "aten::reshape",
        "cuda_mem": 0,
        "cpu_mem": 0
      },
      {
        "name": "aten::t",
        "cuda_mem": 0,
        "cpu_mem": 0
      },
      {
        "name": "aten::addmm",
        "cuda_mem": 0,
        "cpu_mem": 0
      },
      {
        "name": "void gemmSN_TN_kernel<float, 128, 16, 2, 4, 8, 9, false, cublasGemvTensorStridedBatched<float const>, cublasGemvTensorStridedBatched<float const>, cublasGemvTensorStridedBatched<float> >(cublasGemmSmallNParams<cublasGemvTensorStridedBatched<float const>, cublasGemvTensorStridedBatched<float const>, cublasGemvTensorStridedBatched<float>, float>)",
        "cuda_mem": 0,
        "cpu_mem": 0
      }
    ]
  }
}