---
title: "Agnitra Platform"
description: "Production-ready documentation for installing, optimizing, and operating Agnitra."
---

# Agnitra Platform

Agnitra gives your team an opinionated workflow for profiling, optimizing, and shipping large language models with built-in telemetry and billing. The SDK and CLI handle model compilation, surface runtime hotspots, and emit marketplace-ready usage events so you can move from prototype to production without standing up separate infrastructure.

## Platform Pillars

- **Optimization agents** adapt TorchScript/ONNX graphs with LLM + RL tuned kernels so you keep model accuracy while boosting throughput.
- **Telemetry-first workflows** capture latency, GPU hours, and savings in structured JSON so finance and infra teams share the same source of truth.
- **Usage-based monetization** maps optimization runs to Stripe, AWS Marketplace, and internal ledgers with auditable metadata and license enforcement.

## Start Building Fast

- Install the SDK from PyPI and ship your first optimized artifact via the [Quickstart](./intro/quickstart).
- Explore repeatable automations and code samples in the [SDK & CLI Guide](./guides/cli-and-sdk).
- Understand what telemetry and usage events look like in practice with the [Telemetry Playbook](./guides/telemetry).

## Architecture Overview

The MVP couples a telemetry collector, FX graph extractor, AI optimizer, kernel generator, and runtime patcher behind a unified CLI and control plane. Read the [Architecture Deep Dive](./intro/architecture) for module responsibilities, data flows, and performance targets drawn from the PRD.

## Key Workflows

- Monitor production uplift and route usage events using the [Telemetry Playbook](./guides/telemetry).
- Connect finance tooling and marketplaces through the [Marketplace & Billing guide](./guides/marketplace).
- Bring the control plane into your infrastructure with the [`agnitra-api` reference](./reference/control-plane).

## Reference Materials

- Configure environments with the [Runtime Configuration reference](./reference/configuration).
- Integrate the OpenAI Responses API using the [Responses API contract](./reference/responses-api).
- Mirror telemetry exports and SDK helpers with code snippets across the repository (see `agnitra/core`, `agnitra/api`, and `cli/`).

## What's Next?

Run `pip install agnitra`, export `AGNITRA_API_KEY`, and launch `agnitra optimize` against the TinyLlama fixture to validate your workstation. From there, follow the linked guides to embed Agnitra into pipelines, dashboards, and billing loops.
