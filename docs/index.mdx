---
title: "Agnitra Platform"
description: "Production-ready documentation for installing, optimizing, and operating Agnitra."
---

# Agnitra Platform

Agnitra gives your team an opinionated workflow for profiling, optimizing, and shipping large language models with built-in telemetry and billing. The SDK and CLI handle model compilation, surface runtime hotspots, and emit marketplace-ready usage events so you can move from prototype to production without standing up separate infrastructure.

## Platform Pillars

- **Optimization agents** adapt TorchScript/ONNX graphs with LLM + RL tuned kernels so you keep model accuracy while boosting throughput.
- **Telemetry-first workflows** capture latency, GPU hours, and savings in structured JSON so finance and infra teams share the same source of truth.
- **Usage-based monetization** maps optimization runs to Stripe, AWS Marketplace, and internal ledgers with auditable metadata and license enforcement.

## Start Building Fast

- Install the SDKs and run your first optimization in the [Quickstart](./intro/quickstart).
- Explore CLI and Python entry points in the [SDK & CLI Guide](./guides/cli-and-sdk).
- Review expected speedups, telemetry artifacts, and billing events in the [Telemetry Playbook](./guides/telemetry).

## Architecture Overview

The MVP couples a telemetry collector, FX graph extractor, AI optimizer, kernel generator, and runtime patcher behind a unified CLI and control plane. Read the [Architecture Deep Dive](./intro/architecture) for module responsibilities, data flows, and performance targets drawn from the PRD.

## Key Workflows

- Automate production releases with the [Publishing Packages checklist](./operations/publishing) and align docs deploys using the [Mintlify workflow](./operations/docs-deployment).
- Connect marketplace and finance tooling with the [Marketplace & Billing guide](./guides/marketplace).
- Operate the control plane via `agnitra-api` endpoints documented in the [Control Plane Reference](./reference/control-plane).

## Reference Materials

- Configure environments with the [Runtime Configuration reference](./reference/configuration).
- Integrate the OpenAI Responses API using the [Responses API contract](./reference/responses-api).
- Mirror telemetry exports and SDK helpers with code snippets across the repository (see `agnitra/core`, `agnitra/api`, and `cli/`).

## What's Next?

Run `pip install -e .[openai,rl,nvml,marketplace]`, set `AGNITRA_API_KEY`, and launch `agnitra optimize` against the TinyLlama fixture to validate your workstation. From there, follow the linked guides to embed Agnitra into pipelines, dashboards, and billing loops.
