---
title: "Python SDK"
description: "Optimize models, collect telemetry, and emit usage events directly from Python."
---

# Python SDK

The Python SDK gives you programmatic access to the same optimization engine that powers the Agnitra CLI. It returns optimized artifacts, telemetry bundles, and usage events in a single call so you can automate the workflow inside notebooks, batch jobs, or production services.

## Installation

```bash
pip install agnitra
# optional extras
pip install agnitra[openai]          # OpenAI Responses API helpers
pip install agnitra[rl]              # Reinforcement-learning tuning stack
pip install agnitra[nvml]            # GPU telemetry via NVIDIA NVML
pip install agnitra[marketplace]     # Stripe, AWS Marketplace, Google Cloud exporters
```

Set your credentials before importing the package:

```bash
export AGNITRA_API_KEY="sk-live-..."
export AGNITRA_API_BASE_URL="https://api.agnitra.ai"
```

## Optimize a model

```python
import torch
from agnitra import optimize

model = torch.jit.load("tinyllama.pt")
sample = torch.randn(1, 16, 64)

result = optimize(
    model=model,
    input_tensor=sample,
    project_id="demo-project",
    target="A100",
    metadata={"initiated_by": "scheduler"}
)

optimized = result.optimized_model
telemetry = result.telemetry
usage = result.usage_event
```

`result.optimized_model` is a TorchScript module you can persist with `optimized.save("tinyllama_optimized.pt")`. `telemetry` exposes `.write_json(path)` and `.as_dict()` helpers for downstream pipelines.

## Supplying inputs

The optimizer needs representative inputs for accurate benchmarking:

- `input_tensor` – pass a concrete example tensor (recommended).
- `input_shape` – supply shape hints; Agnitra synthesizes tensors via `agnitra.sdk.resolve_input_tensor`.
- `input_fn` – defer tensor creation until runtime by providing a callable.

```python
from agnitra.sdk import resolve_input_tensor

input_tensor = resolve_input_tensor(
    model=model,
    shape_hint=(1, 16, 64),
    dtype="float32",
    device="cuda:0",
)

result = optimize(model, input_tensor=input_tensor)
```

## Routing telemetry

```python
from agnitra.telemetry import export_to_http

result = optimize(model, input_tensor=input_tensor)

# Persist locally
result.telemetry.write_json("telemetry.json")

# Stream to observability stack
export_to_http(
    telemetry=result.telemetry,
    url="https://usage.mycompany.dev/agnitra",
    headers={"Authorization": f"Bearer {MY_TOKEN}"},
    timeout=5,
)
```

Other exporters:

- `export_to_kafka(telemetry, topic="agnitra.telemetry")`
- `export_to_s3(telemetry, bucket="agnitra-telemetry", key_prefix="prod/")`
- `export_usage_to_stripe(result.usage_event)`

## Advanced configuration

| Parameter | Description |
| --- | --- |
| `target` | Preferred hardware tier (`A100`, `A10G`, `cpu`). Defaults to autodetect. |
| `disable_rl` | Skip reinforcement-learning passes for faster iterations. |
| `offline` | Avoid control plane calls (enterprise license required). |
| `job_metadata` | Attach arbitrary key/value metadata that flows into telemetry and usage events. |
| `license_org` / `license_seat` | Override license identity when running in shared environments. |

```python
result = optimize(
    model,
    input_tensor,
    target="A10G",
    disable_rl=True,
    offline=True,
    job_metadata={"release": "2024.09"},
    license_org="AcmeLabs",
    license_seat="ci-runner-42",
)
```

## Error handling

Optimization raises typed exceptions such as `OptimizationTimeout`, `LicenseValidationError`, or `TelemetryExportError`. Wrap long-running jobs with retries or fallback logic:

```python
from agnitra.exceptions import OptimizationTimeout

try:
    result = optimize(model, input_tensor)
except OptimizationTimeout:
    # Retry with a smaller batch or route to a CPU target
    result = optimize(model, input_tensor, target="cpu")
```

## Testing and CI

- Use synthetic models or small fixtures to keep CI fast.
- Record telemetry hashes for regression baselines.
- Run `pytest -q` before publishing new optimization recipes or SDK wrappers.

Next up: wire the outputs into analytics and billing by following the [Telemetry & Billing guide](./telemetry).
