---
title: "CLI Automation"
description: "Run optimizations, launch services, and wire Agnitra into CI/CD using the command-line tools."
---

# CLI Automation

The Agnitra CLI mirrors the Python and JavaScript SDKs so teams can compile models, launch the optimization API, and inspect telemetry from any shell or CI runner.

## Install the CLI

The CLI ships with the Python package:

```bash
pip install agnitra
# or editable for local development
pip install -e .[openai,rl,nvml,marketplace]
```

Verify the installation:

```bash
agnitra --version
```

## `agnitra optimize`

Optimize a TorchScript or ONNX model and export artifacts plus telemetry:

```bash
agnitra optimize \
  --model tinyllama.pt \
  --input-shape 1,16,64 \
  --target A100 \
  --output dist/tinyllama_optimized.pt \
  --telemetry-out telemetry.json \
  --job-metadata '{"release":"2024.09","initiated_by":"ci"}'
```

Common flags:

| Flag | Description |
| --- | --- |
| `--device` | Force execution on `cpu`, `cuda:0`, etc. |
| `--disable-rl` | Skip reinforcement-learning passes for quicker iterations. |
| `--timeout` | Fail the job if optimization exceeds the provided seconds. |
| `--offline` | Run without contacting the control plane (enterprise license required). |
| `--require-license` | Abort if license validation cannot be completed. |

## `agnitra-api`

Run the optimization API locally or as part of a self-hosted deployment:

```bash
agnitra-api --host 0.0.0.0 --port 8080 --workers 4
```

Endpoints:

- `POST /optimize` – synchronous optimizations or asynchronous job submission.
- `GET /jobs/{id}` – poll async job status.
- `POST /usage` – ingest precomputed usage events for billing.

Use `AGNITRA_API_KEY` and `AGNITRA_CONTROL_PLANE_URL` to point the service at your staging or production control plane.

## `agnitra-dashboard`

Render telemetry snapshots locally:

```bash
agnitra-dashboard --telemetry telemetry.json --host 127.0.0.1 --port 3000
```

Supports live reload when fed a directory of telemetry bundles, letting you inspect regressions across releases.

## Integrating with CI/CD

- **GitHub Actions:** Install dependencies, run `agnitra optimize`, and upload artifacts + telemetry to the build artifacts tab.
- **Self-hosted runners:** Cache pre-compiled models and reuse them across runs to minimize job time.
- **Canary workflows:** Use `--job-metadata` to tag runs with branch, commit, or release channel for downstream analytics.

Example GitHub Actions step:

```yaml
- name: Optimize TinyLLaMA
  run: |
    pip install -e .[nvml,marketplace]
    agnitra optimize \
      --model tinyllama.pt \
      --input-shape 1,16,64 \
      --target A100 \
      --output dist/tinyllama_optimized.pt \
      --telemetry-out telemetry.json \
      --job-metadata '{"commit":"${{ github.sha }}"}'
```

## Troubleshooting

- `ModuleNotFoundError: torch` – ensure PyTorch 2.0+ is installed and CUDA libraries are on the path.
- `LicenseValidationError` – confirm `AGNITRA_LICENSE_ORG` and `AGNITRA_LICENSE_SEAT` match your entitlement.
- `OptimizationTimeout` – lower batch sizes, target CPU, or increase the timeout for complex models.
- `Telemetry export failed` – re-run with `--telemetry-out` and resend manually using the [Telemetry & Billing](./telemetry) guide.

Need scripted workflows or custom integrations? Reach out at `support@agnitra.ai`—we maintain templates for GitHub Actions, GitLab CI, and Argo Workflows.
