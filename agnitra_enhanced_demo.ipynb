{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23b0c448",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/zack-dev-cm/agnitraai/blob/main/agnitra_enhanced_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5819d8",
   "metadata": {
    "id": "9d5819d8"
   },
   "source": [
    "# Agnitra AI: Hardware-Oriented Proof-of-Concept (Enhanced Demo)\n",
    "\n",
    "Agnitra AI is an AI-native runtime optimization platform designed to dynamically boost the performance of AI models on GPUs and other accelerators. The core innovations \u2014 dynamic runtime tuning, cross-vendor abstraction, LLM+RL optimization and a telemetry feedback loop \u2014 differentiate it from existing solutions like TensorRT or cuDNN. This notebook provides an enhanced demonstration of those principles, with a focus on hardware-friendly code generation rather than high-level code synthesis. It integrates the latest OpenAI Codex API and GPT-5 via the new Responses API, simulates reinforcement-learning (RL) based tuning, and shows how a recommender/code-generation engine can scan and improve inefficient code.\n",
    "\n",
    "**Disclaimer:** Real API calls require a valid `OPENAI_API_KEY` and access to the respective models. If the key is not provided or the model name is unavailable, the notebook will log debug messages and skip those steps gracefully."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4256b147",
   "metadata": {
    "id": "4256b147"
   },
   "source": [
    "## Overview of Core Modules and MVP Scope\n",
    "\n",
    "Agnitra\u2019s MVP is composed of several modules designed to work together as a runtime optimization pipeline. The platform captures hardware telemetry, converts the model into an intermediate representation (IR), invokes an AI optimizer (LLMs + RL), and then generates and patches custom kernels. The key components summarised below are based on the project requirements document:\n",
    "\n",
    "- **Telemetry Collector:** uses `torch.profiler` to hook into each layer, capturing CUDA time, tensor shapes,   allocated memory and storing the logs in JSON.\n",
    "- **IR Graph Extractor:** converts the model into an IR using `torch.fx` and attaches telemetry to each node,   producing a structured graph that can be fed into an optimizer.\n",
    "- **LLM-Based Optimizer:** prompts a large language model to suggest tiling/block parameters and other kernel   optimizations based on the telemetry and IR.\n",
    "- **Reinforcement Learning (RL) Agent:** simulates a reward loop and uses PPO to tune parameters such as tile   size, loop unrolling or fusion decisions.\n",
    "- **Kernel Generator & Runtime Patcher:** builds a kernel template engine and dynamically replaces graph nodes   with optimized kernels, including fallback logic.\n",
    "\n",
    "This notebook implements a simplified version of the above modules, emphasising clarity, hardware friendliness, and investor appeal. It also demonstrates a **one-line API** (`optimize_model`) that integrates the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c457e57",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8c457e57",
    "outputId": "dd7fe0a8-ccf8-4a29-89e2-f2b75f918c1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m320.7/320.7 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m187.2/187.2 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install -q --upgrade openai==1.30.2\n",
    "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "!pip install -q stable-baselines3\n",
    "!pip install -q gymnasium\n",
    "!pip install -q triton\n",
    "!pip install -q rich\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a589fab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "try:\n",
    "    from google.colab import userdata  # type: ignore\n",
    "    openai_key = userdata.get(\"OPENAI_API_KEY\")\n",
    "except Exception:\n",
    "    openai_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device_name = torch.cuda.get_device_name(0)\n",
    "    if \"A100\" not in device_name:\n",
    "        print(f\"[WARNING] Expected A100 GPU but found {device_name}\")\n",
    "else:\n",
    "    print(\"[WARNING] CUDA not available; running on CPU\")\n",
    "\n",
    "if openai_key:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = openai_key\n",
    "else:\n",
    "    print(\"[WARNING] No OpenAI API key provided.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8746dda1",
   "metadata": {
    "id": "8746dda1"
   },
   "outputs": [],
   "source": [
    "# Imports and API setup\n",
    "import os, json, time, math\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "from torch.fx import symbolic_trace\n",
    "import numpy as np\n",
    "\n",
    "try:\n",
    "    from stable_baselines3 import PPO\n",
    "    import gymnasium as gym\n",
    "    from gymnasium import spaces\n",
    "except Exception as e:\n",
    "    PPO = None\n",
    "    print('[WARNING] RL libraries not available:', e)\n",
    "try:\n",
    "    from openai import OpenAI\n",
    "except ImportError:\n",
    "    OpenAI = None\n",
    "    print('[WARNING] The openai package is not installed.')\n",
    "\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "if OPENAI_API_KEY and OpenAI is not None:\n",
    "    client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "else:\n",
    "    client = None\n",
    "    print('[INFO] OpenAI API key not provided; LLM calls will be skipped.')\n",
    "CODEX_MODEL = 'codex-latest'\n",
    "GPT_MODEL = 'gpt-5'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b2872fd",
   "metadata": {
    "id": "3b2872fd"
   },
   "outputs": [],
   "source": [
    "# 1. Telemetry Collection\n",
    "def collect_telemetry(model: nn.Module, input_tensor: torch.Tensor):\n",
    "    \"\"\"Run a single forward pass with torch.profiler and return telemetry as a list of dicts.\"\"\"\n",
    "    activities = [ProfilerActivity.CPU]\n",
    "    if torch.cuda.is_available():\n",
    "        activities.append(ProfilerActivity.CUDA)\n",
    "    telemetry = []\n",
    "    with profile(activities=activities, record_shapes=True, profile_memory=True) as prof:\n",
    "        with record_function('model_inference'):\n",
    "            _ = model(input_tensor)\n",
    "    for evt in prof.key_averages():\n",
    "        entry = {\n",
    "            'name': evt.key,\n",
    "            'cpu_time_ms': getattr(evt, 'cpu_time_total', 0.0) / 1e6,\n",
    "            'cuda_time_ms': (getattr(evt, 'cuda_time_total', 0.0) / 1e6) if torch.cuda.is_available() else 0.0,\n",
    "            'input_shape': getattr(evt, 'input_shapes', []),\n",
    "            'cpu_memory_bytes': getattr(evt, 'self_cpu_memory_usage', 0),\n",
    "            'cuda_memory_bytes': getattr(evt, 'self_cuda_memory_usage', 0) if torch.cuda.is_available() else 0\n",
    "        }\n",
    "        telemetry.append(entry)\n",
    "    return telemetry\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bae0fb61",
   "metadata": {
    "id": "bae0fb61"
   },
   "outputs": [],
   "source": [
    "# 2. IR Graph Extraction\n",
    "def extract_ir(model: nn.Module, telemetry: list):\n",
    "    \"\"\"Trace the model with torch.fx and attach telemetry to each node.\"\"\"\n",
    "    traced = symbolic_trace(model)\n",
    "    ir_nodes = []\n",
    "    for node in traced.graph.nodes:\n",
    "        matched = None\n",
    "        for entry in telemetry:\n",
    "            if node.target and node.target in str(entry['name']):\n",
    "                matched = entry\n",
    "                break\n",
    "        ir_nodes.append({\n",
    "            'op': node.op,\n",
    "            'target': str(node.target),\n",
    "            'args': str(node.args),\n",
    "            'kwargs': str(node.kwargs),\n",
    "            'telemetry': matched\n",
    "        })\n",
    "    return ir_nodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "443867ef",
   "metadata": {
    "id": "443867ef"
   },
   "outputs": [],
   "source": [
    "# 3. LLM-based optimization suggestion\n",
    "def request_kernel_suggestions(telemetry: list, ir_nodes: list, client=None, model_name=CODEX_MODEL):\n",
    "    \"\"\"Call the LLM to propose optimized kernel parameters. Returns a suggestion or None.\"\"\"\n",
    "    if client is None or OpenAI is None:\n",
    "        print('[INFO] No OpenAI client or API key; skipping kernel suggestion.')\n",
    "        return None\n",
    "    try:\n",
    "        ir_json = json.dumps(ir_nodes)\n",
    "    except TypeError:\n",
    "        ir_json = json.dumps([{'op': n['op'], 'target': n['target']} for n in ir_nodes])\n",
    "    system_message = {\n",
    "        'role': 'system',\n",
    "        'content': [\n",
    "            {'type': 'input_text', 'text': 'You are an expert GPU kernel optimizer. Given telemetry and an IR graph, suggest block size, tile size and unroll factors to reduce latency.'}\n",
    "        ]\n",
    "    }\n",
    "    user_message = {\n",
    "        'role': 'user',\n",
    "        'content': [\n",
    "            {'type': 'input_text', 'text': f\"\"\"Telemetry: {telemetry} IR graph: {ir_json}\n",
    "Provide optimized kernel parameters (e.g., tile sizes, block size, loop unroll count) and rationale.\"\"\"}\n",
    "        ]\n",
    "    }\n",
    "    try:\n",
    "        response = client.responses.create(model=model_name, input=[system_message, user_message], max_output_tokens=1024, store=False)\n",
    "        optimized_text = ''\n",
    "        for item in response.output:\n",
    "            if not hasattr(item, 'content') or item.content is None:\n",
    "                continue\n",
    "            for entry in item.content:\n",
    "                optimized_text += entry.text\n",
    "        return optimized_text.strip()\n",
    "    except Exception as e:\n",
    "        print('[ERROR] LLM call failed:', e)\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d9bdd88",
   "metadata": {
    "id": "7d9bdd88"
   },
   "outputs": [],
   "source": [
    "# 4. Code scanning and improvement via GPT\n",
    "def improve_python_code(code_str: str, client=None, model_name=GPT_MODEL):\n",
    "    \"\"\"Send a code snippet to the GPT model and request an improved, hardware-friendly version.\"\"\"\n",
    "    if client is None or OpenAI is None:\n",
    "        print('[INFO] No OpenAI client or API key; skipping code improvement.')\n",
    "        return None\n",
    "    system_message = {\n",
    "        'role': 'system',\n",
    "        'content': [\n",
    "            {'type': 'input_text', 'text': 'You are a senior performance engineer. Improve the following Python code by making it more hardware-efficient (vectorized operations, batching).'}\n",
    "        ]\n",
    "    }\n",
    "    user_message = {\n",
    "        'role': 'user',\n",
    "        'content': [\n",
    "            {'type': 'input_text', 'text': code_str}\n",
    "        ]\n",
    "    }\n",
    "    try:\n",
    "        response = client.responses.create(model=model_name, input=[system_message, user_message], max_output_tokens=1024, store=False)\n",
    "        improved = ''\n",
    "        for item in response.output:\n",
    "            if item.content is None:\n",
    "                continue\n",
    "            for chunk in item.content:\n",
    "                improved += chunk.text\n",
    "        return improved.strip()\n",
    "    except Exception as e:\n",
    "        print('[ERROR] Code improvement call failed:', e)\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f6c39830",
   "metadata": {
    "id": "f6c39830"
   },
   "outputs": [],
   "source": [
    "# 5. Reinforcement Learning Environment for Kernel Tuning\n",
    "class KernelTuningEnv(gym.Env):\n",
    "    \"\"\"A simple environment where the action controls tile size and unroll factor for matrix multiplication.\"\"\"\n",
    "    def __init__(self, matrix_size=256):\n",
    "        super().__init__()\n",
    "        self.param_options = [(16,1),(32,1),(64,1),(128,1),(16,2),(32,2),(64,2)]\n",
    "        self.action_space = spaces.Discrete(len(self.param_options))\n",
    "        self.observation_space = spaces.Box(low=0.0, high=1.0, shape=(1,), dtype=float)\n",
    "        self.size = matrix_size\n",
    "        self.A = np.random.rand(self.size, self.size).astype(np.float32)\n",
    "        self.B = np.random.rand(self.size, self.size).astype(np.float32)\n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        return np.array([0.0], dtype=float), {}\n",
    "    def step(self, action):\n",
    "        tile_size, unroll = self.param_options[int(action)]\n",
    "        start = time.perf_counter()\n",
    "        result = np.zeros((self.size, self.size), dtype=np.float32)\n",
    "        for i in range(0, self.size, tile_size):\n",
    "            for j in range(0, self.size, tile_size):\n",
    "                for k in range(0, self.size, tile_size):\n",
    "                    i_end, j_end, k_end = i+tile_size, j+tile_size, k+tile_size\n",
    "                    block = np.dot(self.A[i:i_end, k:k_end], self.B[k:k_end, j:j_end])\n",
    "                    for _ in range(unroll):\n",
    "                        result[i:i_end, j:j_end] += block\n",
    "        elapsed = time.perf_counter() - start\n",
    "        reward = -elapsed\n",
    "        return np.array([0.0], dtype=float), reward, True, False, {'tile_size': tile_size, 'unroll': unroll, 'elapsed': elapsed}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec9b865f",
   "metadata": {
    "id": "ec9b865f"
   },
   "outputs": [],
   "source": [
    "# 6. High-level wrapper for model optimization\n",
    "\n",
    "def optimize_model(model: nn.Module, input_tensor: torch.Tensor, enable_rl: bool = True):\n",
    "    \"\"\"One-line API that profiles a model, extracts IR, obtains LLM suggestions and optionally tunes parameters with RL.\"\"\"\n",
    "    telemetry = collect_telemetry(model, input_tensor)\n",
    "    print(f'[INFO] Collected {len(telemetry)} telemetry events.')\n",
    "    ir_nodes = extract_ir(model, telemetry)\n",
    "    print(f'[INFO] Extracted {len(ir_nodes)} IR nodes.')\n",
    "    suggestion = request_kernel_suggestions(telemetry, ir_nodes, client=client)\n",
    "    if suggestion:\n",
    "        print(\"\"\"[LLM Suggestion]\"\"\" + suggestion)\n",
    "    else:\n",
    "        print('[LLM Suggestion] No suggestion or API unavailable.')\n",
    "    if enable_rl and PPO is not None:\n",
    "        env = KernelTuningEnv(matrix_size=128)\n",
    "        model_rl = PPO('MlpPolicy', env, verbose=0, n_steps=1, batch_size=4, ent_coef=0.0, n_epochs=1)\n",
    "        model_rl.learn(total_timesteps=20)\n",
    "        _, _ = env.reset()\n",
    "        best_reward, best_params = -math.inf, None\n",
    "        for action_idx in range(env.action_space.n):\n",
    "            _, reward, _, _, meta = env.step(action_idx)\n",
    "            print(f'    RL eval: tile_size={meta[\"tile_size\"]}, unroll={meta[\"unroll\"]}, elapsed={meta[\"elapsed\"]:.4f}s, reward={reward:.4f}')\n",
    "            if reward > best_reward:\n",
    "                best_reward, best_params = reward, (meta['tile_size'], meta['unroll'])\n",
    "        print(f'[RL Tuner] Best parameters found: tile_size={best_params[0]}, unroll={best_params[1]}')\n",
    "    else:\n",
    "        print('[RL Tuner] Skipped due to missing dependencies or disabled.')\n",
    "    return {'telemetry': telemetry, 'ir': ir_nodes, 'llm_suggestion': suggestion}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e7a54e9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 385
    },
    "id": "4e7a54e9",
    "outputId": "470fb97d-0612-4a31-847b-4c42f57dfe38"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'FunctionEventAvg' object has no attribute 'cuda_time_total'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-2528251857.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mdemo_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdemo_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0minput_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdemo_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mresult_meta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimize_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdemo_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipython-input-3789416198.py\u001b[0m in \u001b[0;36moptimize_model\u001b[0;34m(model, input_tensor, enable_rl)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0moptimize_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menable_rl\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m\"\"\"One-line API that profiles a model, extracts IR, obtains LLM suggestions and optionally tunes parameters with RL.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtelemetry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollect_telemetry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'[INFO] Collected {len(telemetry)} telemetry events.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mir_nodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_ir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtelemetry\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipython-input-2587168401.py\u001b[0m in \u001b[0;36mcollect_telemetry\u001b[0;34m(model, input_tensor)\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0;34m'name'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mevt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;34m'cpu_time_ms'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mevt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu_time_total\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m1e6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0;34m'cuda_time_ms'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mevt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda_time_total\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m1e6\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m             \u001b[0;34m'input_shape'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mevt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0;34m'cpu_memory_bytes'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mevt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mself_cpu_memory_usage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'FunctionEventAvg' object has no attribute 'cuda_time_total'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:151: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.utcnow().replace(tzinfo=utc)\n"
     ]
    }
   ],
   "source": [
    "# 7. Demonstration on a simple model\n",
    "\n",
    "class DemoNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(512, 512)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(512, 512)\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))\n",
    "\n",
    "demo_model = DemoNet()\n",
    "demo_device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "demo_model.to(demo_device)\n",
    "input_tensor = torch.randn(1, 512, device=demo_device)\n",
    "\n",
    "# Usage\n",
    "result_meta = optimize_model(demo_model, input_tensor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809126fe",
   "metadata": {
    "id": "809126fe"
   },
   "source": [
    "## Investor Use-Cases and Business Model\n",
    "\n",
    "Agnitra AI is designed to serve multiple customer segments, each with a clear value proposition. According to the PRD, the target customers and their needs include ML teams seeking faster inference and cheaper deployment, chip companies demonstrating better performance-per-dollar, cloud providers aiming to improve GPU utilisation, AI startups looking to run LLMs on lower-cost hardware, and OEMs wanting embedded runtime optimisation. The business model mixes B2B SaaS, enterprise SDK licensing, per-GPU licensing and an optimisation-as-a-service offering. The one-line API demonstrated above (`optimize_model`) illustrates how quickly developers can adopt the technology, aligning with the goal of a sub-10-minute SDK integration time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5714e5c",
   "metadata": {
    "id": "e5714e5c"
   },
   "source": [
    "## Initial Focus Features and Scaling Roadmap\n",
    "\n",
    "The MVP emphasises the telemetry collector, IR extractor, prompt-based LLM optimiser, RL agent, kernel generator and runtime patcher. These foundational components enable Agnitra to perform runtime tuning without requiring model code changes. Post-MVP phases will add a telemetry visualisation dashboard, support for alternative compiler backends, multi-vendor hardware support and a hosted optimisation-as-a-service platform."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d796d81",
   "metadata": {
    "id": "3d796d81"
   },
   "source": [
    "## Conclusion and Next Steps\n",
    "\n",
    "This notebook demonstrated a hardware-oriented proof of concept for Agnitra AI with an integrated pipeline: it collected telemetry, converted the model to an IR, queried the latest Codex/GPT models for optimisation suggestions, simulated RL-based tuning and even scanned and improved inefficient Python code. By exposing everything through a single function (`optimize_model`), developers can adopt Agnitra quickly and benefit from dynamic tuning and hardware-aware code generation.\n",
    "\n",
    "To advance this prototype, one could integrate real Triton kernels, extend the RL environment to tune additional parameters, and package the modules into an installable `agnitra` library and CLI. Coupled with the business model and roadmap, these enhancements would position Agnitra as a compelling investment opportunity in the AI infrastructure space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0d83ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from agnitra.sdk import optimizer as sdk_optimizer\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "class TinyNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = torch.nn.Linear(4, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = TinyNet().to(device)\n",
    "input_tensor = torch.randn(1, 4, device=device)\n",
    "telemetry = sdk_optimizer.collect_telemetry(model, input_tensor)\n",
    "print('Telemetry:', telemetry)\n",
    "ir_nodes = sdk_optimizer.extract_ir(model, telemetry)\n",
    "suggestion = sdk_optimizer.request_kernel_suggestions(telemetry, ir_nodes)\n",
    "print('LLM suggestion:', suggestion)\n",
    "sdk_optimizer.optimize_model(model, input_tensor, enable_rl=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "profile-sample-models",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agnitra.demo.demo import profile_sample_models\n",
    "profile_sample_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d40a1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pytest -k \"optimizer or telemetry\""
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}