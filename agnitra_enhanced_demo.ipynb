{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {"tags": ["colab-setup"]}, "outputs": [], "source": ["# Base setup: Colab secrets, repo clone, env vars\n", "import os, pathlib\n", "try:\n", "    from google.colab import userdata  # type: ignore\n", "    GH_TOKEN = userdata.get('GH_TOKEN')\n", "    OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n", "    GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n", "    AGNITRAAI_REPO = userdata.get('AGNITRAAI_REPO')\n", "    if OPENAI_API_KEY: os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY\n", "    if GOOGLE_API_KEY: os.environ['GOOGLE_API_KEY'] = GOOGLE_API_KEY\n", "    repo_path = pathlib.Path('/content/agnitraai')\n", "    if pathlib.Path('/content').exists() and not repo_path.exists() and GH_TOKEN and AGNITRAAI_REPO:\n", "        repo_url = f'https://{GH_TOKEN}@{AGNITRAAI_REPO}'\n", "        get_ipython().system(f'git clone {repo_url} {repo_path}')\n", "        os.chdir(repo_path)\n", "    elif repo_path.exists():\n", "        os.chdir(repo_path)\n", "except Exception:\n", "    pass\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": ["install-cli"]}, "outputs": [], "source": ["# Install Agnitra locally so `agnitra` CLI is available in Colab\n", "import pathlib, os\n", "repo_path = pathlib.Path('/content/agnitraai')\n", "if repo_path.exists():\n", "    get_ipython().system('python -m pip install -q -e /content/agnitraai')\n", "else:\n", "    get_ipython().system('python -m pip install -q -e .')\n", "# Quick sanity: show help (fallback to module if console script not found)\n", "_ = get_ipython().system('agnitra --help >/dev/null 2>&1 || python -m agnitraai.cli.main --help >/dev/null 2>&1')\n", "print('Agnitra CLI available.')\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Agnitra CLI Demo â€” TinyLlama\n", "\n", "This notebook demonstrates a clean, CLI-first workflow:\n", "- Prepare a tiny TinyLlama-like model\n", "- Run profiling via `agnitra profile ...` (CPU/GPU auto-detected)\n", "- Load telemetry artifacts and visualize key results\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Minimal environment check and CLI availability\n", "import shutil, subprocess, sys, json\n", "try:\n", "    import torch\n", "    print(f'[Env] CUDA available: {torch.cuda.is_available()}')\n", "    if torch.cuda.is_available():\n", "        print('[Env] GPU:', torch.cuda.get_device_name(0))\n", "    else:\n", "        print('[Env] CPU-only runtime')\n", "except Exception as e:\n", "    print('[Env] Torch check skipped:', e)\n", "\n", "if shutil.which('agnitra') is None:\n", "    print('[Setup] Installing local CLI ...')\n", "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', '-e', 'agnitraai'])\n", "else:\n", "    print('[Setup] agnitra CLI found')\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Prepare tiny model\n", "Generates `tinyllama.pt` (TorchScript) and a small info JSON."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import subprocess, sys\n", "cmd = [sys.executable, 'prepare_tinyllama.py']\n", "print('$', ' '.join(cmd))\n", "subprocess.check_call(cmd)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Profile using the Agnitra CLI\n", "Runs the CLI and prints only the CLI's stdout for a clean view."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import os, shlex, subprocess, shutil, sys\n", "env = os.environ.copy()\n", "env['TF_CPP_MIN_LOG_LEVEL'] = '3'\n", "env['GYM_DISABLE_WARNINGS'] = '1'\n", "base_cmd = 'agnitra profile tinyllama.pt --input-shape 1,16,64 --output telemetry.json'\n", "if shutil.which('agnitra') is None:\n", "    base_cmd = f'{sys.executable} -m cli.main profile tinyllama.pt --input-shape 1,16,64 --output telemetry.json'\n", "print('$', base_cmd)\n", "res = subprocess.run(shlex.split(base_cmd), env=env, check=True, capture_output=True, text=True)\n", "print(res.stdout)  # keep CLI output clean (stderr suppressed)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Load telemetry and summarize"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from pathlib import Path\n", "import json\n", "root = Path('agnitraai')/ 'context'\n", "telemetry = json.loads(Path('telemetry.json').read_text())\n", "train = json.loads(Path('telemetry_train.json').read_text()) if Path('telemetry_train.json').exists() else {'events': []}\n", "summary = json.loads((root / 'profile_result_tinyllama.json').read_text())\n", "\n", "def to_ms_ns(v):\n", "    try: return float(v)/1e6\n", "    except: return 0.0\n", "\n", "total_cpu_ms = sum(to_ms_ns(e.get('cpu_time_total',0.0)) for e in telemetry.get('events', []))\n", "total_cuda_ms = sum(to_ms_ns(e.get('cuda_time_total',0.0)) for e in telemetry.get('events', []))\n", "print('[Summary] device:', summary.get('device'))\n", "print('[Summary] events:', len(telemetry.get('events', [])))\n", "print('[Summary] CPU total ms (inference):', round(total_cpu_ms, 3))\n", "print('[Summary] CUDA total ms (inference):', round(total_cuda_ms, 3))\n", "print('[Summary] training events:', len(train.get('events', [])))\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Visualize top events"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import matplotlib.pyplot as plt\n", "from collections import defaultdict\n", "ev = telemetry.get('events', [])\n", "def top(evts, key, n=10):\n", "    return sorted(evts, key=lambda e: e.get(key, 0.0), reverse=True)[:n]\n", "# Prefer CPU on CPU-only runs\n", "top_cpu = top(ev, 'cpu_time_total')\n", "names = [e.get('name','') for e in top_cpu]\n", "vals = [e.get('cpu_time_total',0.0)/1e6 for e in top_cpu]\n", "plt.figure(figsize=(10,4))\n", "plt.barh(names[::-1], vals[::-1])\n", "plt.xlabel('CPU time (ms)')\n", "plt.title('Top ops by CPU time (inference)')\n", "plt.tight_layout(); plt.show()\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Artifact paths"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from pathlib import Path\n", "out = [\n", "    'telemetry.json',\n", "    'telemetry_train.json',\n", "    'agnitraai/context/layer_log_tinyllama.json',\n", "    'agnitraai/context/profile_result_tinyllama.json',\n", "]\n", "for p in out:\n", "    print('[Artifact]', p, 'exists:', Path(p).exists())\n"]}], "metadata": {"accelerator": "GPU", "kernelspec": {"display_name": "Python 3", "name": "python3"}}, "nbformat": 4, "nbformat_minor": 5}